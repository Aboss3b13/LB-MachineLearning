{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0691af7f",
   "metadata": {},
   "source": [
    "**3.1 Split the dataset into training and test sets**\n",
    "I start by loading and cleaning the dataset. Then, we split the data: 80% for training, 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ea611",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_ml.drop(columns=[\"price\"])\n",
    "y = df_ml[\"price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e889f73",
   "metadata": {},
   "source": [
    "**3.2 Algorithm Selection and Training**\n",
    "I chose the RandomForestRegressor from sklearn.ensemble for this problem because it's well-suited to datasets like housing data, which often include nonlinear relationships between features like square footage, number of bedrooms, and lot size. Random forests are robust against outliers and can handle both numerical and categorical features with minimal preprocessing. Unlike simple linear models, they can model complex interactions automatically. I also considered other models like linear regression, but Random Forest tends to perform better on real-world tabular data, especially when we’re not sure which features interact strongly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05da3ec3-e5cb-411e-9a00-2bdadd21472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c14701",
   "metadata": {},
   "source": [
    "**3.3 Reflection**\n",
    "\n",
    "My model estimated the house price to be about $504,169.55, while the actual listing price was $429,000. That’s a difference of around $75,000, which suggests the model overestimated in this case. This might be due to missing features like the property's condition or renovation status, which strongly influence price. Still, the prediction isn’t wildly off. To improve accuracy, I could add more local market features, newer data, or try a different algorithm like XGBoost for better performance on edge cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140ba881",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Actual\": y_test.values,\n",
    "    \"Predicted\": y_pred.round(2)\n",
    "})\n",
    "print(comparison.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
